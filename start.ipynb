{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPjCb941iF0r"
      },
      "source": [
        "# Recreation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLW_I8sViF0s"
      },
      "source": [
        "## Logistic Regression with 7 Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ucNOVpiF0t",
        "outputId": "800149e5-1ca9-43cd-832d-8a2e3723aceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7214 entries, 0 to 7213\n",
            "Data columns (total 53 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       7214 non-null   int64  \n",
            " 1   name                     7214 non-null   object \n",
            " 2   first                    7214 non-null   object \n",
            " 3   last                     7214 non-null   object \n",
            " 4   compas_screening_date    7214 non-null   object \n",
            " 5   sex                      7214 non-null   object \n",
            " 6   dob                      7214 non-null   object \n",
            " 7   age                      7214 non-null   int64  \n",
            " 8   age_cat                  7214 non-null   object \n",
            " 9   race                     7214 non-null   object \n",
            " 10  juv_fel_count            7214 non-null   int64  \n",
            " 11  decile_score             7214 non-null   int64  \n",
            " 12  juv_misd_count           7214 non-null   int64  \n",
            " 13  juv_other_count          7214 non-null   int64  \n",
            " 14  priors_count             7214 non-null   int64  \n",
            " 15  days_b_screening_arrest  6907 non-null   float64\n",
            " 16  c_jail_in                6907 non-null   object \n",
            " 17  c_jail_out               6907 non-null   object \n",
            " 18  c_case_number            7192 non-null   object \n",
            " 19  c_offense_date           6055 non-null   object \n",
            " 20  c_arrest_date            1137 non-null   object \n",
            " 21  c_days_from_compas       7192 non-null   float64\n",
            " 22  c_charge_degree          7214 non-null   object \n",
            " 23  c_charge_desc            7185 non-null   object \n",
            " 24  is_recid                 7214 non-null   int64  \n",
            " 25  r_case_number            3471 non-null   object \n",
            " 26  r_charge_degree          3471 non-null   object \n",
            " 27  r_days_from_arrest       2316 non-null   float64\n",
            " 28  r_offense_date           3471 non-null   object \n",
            " 29  r_charge_desc            3413 non-null   object \n",
            " 30  r_jail_in                2316 non-null   object \n",
            " 31  r_jail_out               2316 non-null   object \n",
            " 32  violent_recid            0 non-null      float64\n",
            " 33  is_violent_recid         7214 non-null   int64  \n",
            " 34  vr_case_number           819 non-null    object \n",
            " 35  vr_charge_degree         819 non-null    object \n",
            " 36  vr_offense_date          819 non-null    object \n",
            " 37  vr_charge_desc           819 non-null    object \n",
            " 38  type_of_assessment       7214 non-null   object \n",
            " 39  decile_score.1           7214 non-null   int64  \n",
            " 40  score_text               7214 non-null   object \n",
            " 41  screening_date           7214 non-null   object \n",
            " 42  v_type_of_assessment     7214 non-null   object \n",
            " 43  v_decile_score           7214 non-null   int64  \n",
            " 44  v_score_text             7214 non-null   object \n",
            " 45  v_screening_date         7214 non-null   object \n",
            " 46  in_custody               6978 non-null   object \n",
            " 47  out_custody              6978 non-null   object \n",
            " 48  priors_count.1           7214 non-null   int64  \n",
            " 49  start                    7214 non-null   int64  \n",
            " 50  end                      7214 non-null   int64  \n",
            " 51  event                    7214 non-null   int64  \n",
            " 52  two_year_recid           7214 non-null   int64  \n",
            "dtypes: float64(4), int64(16), object(33)\n",
            "memory usage: 2.9+ MB\n"
          ]
        }
      ],
      "source": [
        "# loading data and general information\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_compas = pd.read_csv('compas-scores-two-years.csv')\n",
        "\n",
        "df_compas.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "4LYxD9LoiF0u",
        "outputId": "28473b5d-3b43-4d54-e7b3-a1b380372349"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>compas_screening_date</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>age</th>\n",
              "      <th>age_cat</th>\n",
              "      <th>race</th>\n",
              "      <th>...</th>\n",
              "      <th>v_decile_score</th>\n",
              "      <th>v_score_text</th>\n",
              "      <th>v_screening_date</th>\n",
              "      <th>in_custody</th>\n",
              "      <th>out_custody</th>\n",
              "      <th>priors_count.1</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>event</th>\n",
              "      <th>two_year_recid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>miguel hernandez</td>\n",
              "      <td>miguel</td>\n",
              "      <td>hernandez</td>\n",
              "      <td>2013-08-14</td>\n",
              "      <td>Male</td>\n",
              "      <td>1947-04-18</td>\n",
              "      <td>69</td>\n",
              "      <td>Greater than 45</td>\n",
              "      <td>Other</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Low</td>\n",
              "      <td>2013-08-14</td>\n",
              "      <td>2014-07-07</td>\n",
              "      <td>2014-07-14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>327</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>kevon dixon</td>\n",
              "      <td>kevon</td>\n",
              "      <td>dixon</td>\n",
              "      <td>2013-01-27</td>\n",
              "      <td>Male</td>\n",
              "      <td>1982-01-22</td>\n",
              "      <td>34</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>African-American</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Low</td>\n",
              "      <td>2013-01-27</td>\n",
              "      <td>2013-01-26</td>\n",
              "      <td>2013-02-05</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>159</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>ed philo</td>\n",
              "      <td>ed</td>\n",
              "      <td>philo</td>\n",
              "      <td>2013-04-14</td>\n",
              "      <td>Male</td>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>24</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>African-American</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>Low</td>\n",
              "      <td>2013-04-14</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>marcu brown</td>\n",
              "      <td>marcu</td>\n",
              "      <td>brown</td>\n",
              "      <td>2013-01-13</td>\n",
              "      <td>Male</td>\n",
              "      <td>1993-01-21</td>\n",
              "      <td>23</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>African-American</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>Medium</td>\n",
              "      <td>2013-01-13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1174</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>bouthy pierrelouis</td>\n",
              "      <td>bouthy</td>\n",
              "      <td>pierrelouis</td>\n",
              "      <td>2013-03-26</td>\n",
              "      <td>Male</td>\n",
              "      <td>1973-01-22</td>\n",
              "      <td>43</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>Other</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Low</td>\n",
              "      <td>2013-03-26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>marsha miles</td>\n",
              "      <td>marsha</td>\n",
              "      <td>miles</td>\n",
              "      <td>2013-11-30</td>\n",
              "      <td>Male</td>\n",
              "      <td>1971-08-22</td>\n",
              "      <td>44</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>Other</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Low</td>\n",
              "      <td>2013-11-30</td>\n",
              "      <td>2013-11-30</td>\n",
              "      <td>2013-12-01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>853</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows Ã— 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                name   first         last compas_screening_date   sex  \\\n",
              "0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
              "1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
              "2   4            ed philo      ed        philo            2013-04-14  Male   \n",
              "3   5         marcu brown   marcu        brown            2013-01-13  Male   \n",
              "4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
              "5   7        marsha miles  marsha        miles            2013-11-30  Male   \n",
              "\n",
              "          dob  age          age_cat              race  ...  v_decile_score  \\\n",
              "0  1947-04-18   69  Greater than 45             Other  ...               1   \n",
              "1  1982-01-22   34          25 - 45  African-American  ...               1   \n",
              "2  1991-05-14   24     Less than 25  African-American  ...               3   \n",
              "3  1993-01-21   23     Less than 25  African-American  ...               6   \n",
              "4  1973-01-22   43          25 - 45             Other  ...               1   \n",
              "5  1971-08-22   44          25 - 45             Other  ...               1   \n",
              "\n",
              "   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
              "0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n",
              "1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
              "2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
              "3        Medium        2013-01-13         NaN          NaN               1   \n",
              "4           Low        2013-03-26         NaN          NaN               2   \n",
              "5           Low        2013-11-30  2013-11-30   2013-12-01               0   \n",
              "\n",
              "  start   end event two_year_recid  \n",
              "0     0   327     0              0  \n",
              "1     9   159     1              1  \n",
              "2     0    63     0              1  \n",
              "3     0  1174     0              0  \n",
              "4     0  1102     0              0  \n",
              "5     1   853     0              0  \n",
              "\n",
              "[6 rows x 53 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# inspection of the first 6 rows of the dataset\n",
        "df_compas.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XbRNw9iiF0v",
        "outputId": "48863105-5c41-47f9-a1b2-88457a008cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
            "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
            "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
            "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
            "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
            "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
            "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
            "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
            "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
            "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
            "       'decile_score.1', 'score_text', 'screening_date',\n",
            "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
            "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
            "       'start', 'end', 'event', 'two_year_recid'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# available labels for each row\n",
        "print(df_compas.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GUwwFM7BiF0w"
      },
      "outputs": [],
      "source": [
        "# removing unused columns\n",
        "df_LR7 = df_compas.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'race', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# transforming string values into numerical\n",
        "df_LR7['sex'] = df_LR7['sex'].astype('category')\n",
        "df_LR7['sex'] = df_LR7['sex'].cat.codes\n",
        "\n",
        "df_LR7['c_charge_degree'] = df_LR7['c_charge_degree'].astype('category')\n",
        "df_LR7['c_charge_degree'] = df_LR7['c_charge_degree'].cat.codes\n",
        "\n",
        "df_LR7['c_charge_desc'] = df_LR7['c_charge_desc'].astype('category')\n",
        "df_LR7['c_charge_desc'] = df_LR7['c_charge_desc'].cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "QiJKiCoTiF0y",
        "outputId": "6fbeae18-5222-457b-f109-c30a39c73dfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sex                0\n",
              "age                0\n",
              "juv_fel_count      0\n",
              "juv_misd_count     0\n",
              "priors_count       0\n",
              "c_charge_degree    0\n",
              "c_charge_desc      0\n",
              "two_year_recid     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for null values in the data => it has no null values\n",
        "df_LR7.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HTvlbFRAiF0z"
      },
      "outputs": [],
      "source": [
        "# splitting the dataset in features and the variable to predict\n",
        "\n",
        "X_overall_LR7 = df_LR7.drop(columns='two_year_recid')\n",
        "y_overall_LR7 = df_LR7['two_year_recid']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7WZ4XUniF00"
      },
      "source": [
        "#### Overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg2yl1oqiF01",
        "outputId": "f85a7c92-e29f-4307-9f6f-0a691fb45183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy overall: 67.58\n",
            "Mean of the F1-score: 0.6\n",
            "Standard deviation of the F1-score: 0.02\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=28)\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_overall_LR7 = cross_val_score(lr, X_overall_LR7, y_overall_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_overall_LR7 = round(np.average(kfscore_accuracy_overall_LR7) * 100, 2)\n",
        "print('Accuracy overall:', accuracy_overall_LR7)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_overall_LR7_std = round(np.std(kfscore_accuracy_overall_LR7) * 100, 2)\n",
        "table_overall_LR7 = str(accuracy_overall_LR7) + ' [+/-' + str(accuracy_overall_LR7_std) + ']'\n",
        "\n",
        "# F1-score\n",
        "kfscore_f1_overall_LR7 = cross_val_score(lr, X_overall_LR7, y_overall_LR7, cv=kf, scoring='f1')\n",
        "F1_mean_LR7 = round(np.mean(kfscore_f1_overall_LR7), 2)\n",
        "print('Mean of the F1-score:', F1_mean_LR7)\n",
        "\n",
        "# standard deviaton of the F1-score\n",
        "F1_std_LR7 = round(np.std(kfscore_f1_overall_LR7), 2)\n",
        "print('Standard deviation of the F1-score:', F1_std_LR7)\n",
        "table_F1_LR7 = str(F1_mean_LR7) + ' [+/-' + str(F1_std_LR7) + ']'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0njPxTniF02"
      },
      "source": [
        "#### African-American people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUTdybKliF03",
        "outputId": "71060da5-6055-4185-b02b-977f95eda6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Black people: 66.88\n",
            "False Positive for Black people: 37.77\n",
            "False Negative for Black people: 28.72\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# dataset for Black people\n",
        "df_black_LR7 = df_compas.loc[df_compas['race'] == \"African-American\"]\n",
        "df_black_LR7 = df_black_LR7.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'race', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# encoding\n",
        "df_black_LR7['sex'] = df_black_LR7['sex'].astype('category')\n",
        "df_black_LR7['sex'] = df_black_LR7['sex'].cat.codes\n",
        "df_black_LR7['c_charge_degree'] = df_black_LR7['c_charge_degree'].astype('category')\n",
        "df_black_LR7['c_charge_degree'] = df_black_LR7['c_charge_degree'].cat.codes\n",
        "df_black_LR7['c_charge_desc'] = df_black_LR7['c_charge_desc'].astype('category')\n",
        "df_black_LR7['c_charge_desc'] = df_black_LR7['c_charge_desc'].cat.codes\n",
        "\n",
        "# splitting the data\n",
        "X_black_LR7 = df_black_LR7.drop(columns='two_year_recid')\n",
        "y_black_LR7 = df_black_LR7['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_black_LR7 = cross_val_score(lr, X_black_LR7, y_black_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_black_LR7 = round(np.average(kfscore_accuracy_black_LR7) * 100, 2)\n",
        "print('Accuracy for Black people:', accuracy_black_LR7)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_black_LR7_std = round(np.std(kfscore_accuracy_black_LR7) * 100, 2)\n",
        "table_black_LR7 = str(accuracy_black_LR7) + ' [+/-' + str(accuracy_black_LR7_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_black_LR7 = cross_val_predict(lr, X_black_LR7, y_black_LR7, cv=kf)\n",
        "conf_mat_black_LR7 = confusion_matrix(y_black_LR7, y_pred_black_LR7, normalize='true')\n",
        "FP_black_LR7 = round(conf_mat_black_LR7[0,1] * 100, 2)\n",
        "FN_black_LR7 = round(conf_mat_black_LR7[1,0] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_LR7)\n",
        "print('False Negative for Black people:', FN_black_LR7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5miElifLiF04"
      },
      "source": [
        "#### Caucasian people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T1iBLNLiF04",
        "outputId": "f1ec883e-ce22-443f-ec63-1a0bae33cbe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for White people: 67.4\n",
            "False Positive for White people: 11.29\n",
            "False Negative for White people: 65.42\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# dataset for White people \n",
        "df_white_LR7 = df_compas.loc[df_compas['race'] == 'Caucasian']\n",
        "df_white_LR7 = df_white_LR7.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'race', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# encoding\n",
        "df_white_LR7['sex'] = df_white_LR7['sex'].astype('category')\n",
        "df_white_LR7['sex'] = df_white_LR7['sex'].cat.codes\n",
        "df_white_LR7['c_charge_degree'] = df_white_LR7['c_charge_degree'].astype('category')\n",
        "df_white_LR7['c_charge_degree'] = df_white_LR7['c_charge_degree'].cat.codes\n",
        "df_white_LR7['c_charge_desc'] = df_white_LR7['c_charge_desc'].astype('category')\n",
        "df_white_LR7['c_charge_desc'] = df_white_LR7['c_charge_desc'].cat.codes\n",
        "\n",
        "# splitting the data\n",
        "X_white_LR7 = df_white_LR7.drop(columns='two_year_recid')\n",
        "y_white_LR7 = df_white_LR7['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_white_LR7 = cross_val_score(lr, X_white_LR7, y_white_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_white_LR7 = round(np.average(kfscore_accuracy_white_LR7) * 100, 2)\n",
        "print('Accuracy for White people:', accuracy_white_LR7)\n",
        "accuracy_white_LR7_std = round(np.std(kfscore_accuracy_overall_LR7) * 100, 2)\n",
        "table_white_LR7 = str(accuracy_overall_LR7) + ' [+/-' + str(accuracy_overall_LR7_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_white_LR7 = cross_val_predict(lr, X_white_LR7, y_white_LR7, cv=kf)\n",
        "conf_mat_white_LR7 = confusion_matrix(y_white_LR7, y_pred_white_LR7, normalize='true')\n",
        "FP_white_LR7 = round(conf_mat_white_LR7[0,1] * 100, 2)\n",
        "FN_white_LR7 = round(conf_mat_white_LR7[1,0] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_LR7)\n",
        "print('False Negative for White people:', FN_white_LR7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Percentages within the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of Black people: 51.23%\n",
            "Percentage of White people: 34.02%\n",
            "Percentage of female people: 19.34%\n",
            "Percentage of male people: 80.66%\n"
          ]
        }
      ],
      "source": [
        "# Black people\n",
        "percentage_black = round(len(df_black_LR7) / len(df_compas) * 100, 2)\n",
        "print(\"Percentage of Black people: \" + str(percentage_black) + \"%\")\n",
        "\n",
        "# White people\n",
        "percentage_white = round(len(df_white_LR7) / len(df_compas) * 100, 2)\n",
        "print(\"Percentage of White people: \" + str(percentage_white) + \"%\")\n",
        "\n",
        "# female\n",
        "df_female = df_compas.loc[df_compas['sex'] == \"Female\"]\n",
        "percentage_female = round(len(df_female) / len(df_compas) * 100, 2)\n",
        "print(\"Percentage of female people: \" + str(percentage_female) + \"%\")\n",
        "\n",
        "# male\n",
        "df_male = df_compas.loc[df_compas['sex'] == \"Male\"]\n",
        "percentage_male = round(len(df_male) / len(df_compas) * 100, 2)\n",
        "print(\"Percentage of male people: \" + str(percentage_male) + \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-DAylCuiF04"
      },
      "source": [
        "## Logistic Regression with 2 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9la56gBiF05"
      },
      "source": [
        "#### Overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1fW6-triF05",
        "outputId": "ccec2a01-74c4-4fa2-dee5-c68f59b55cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy overall: 67.51\n",
            "Mean of the F1-score: 0.58\n",
            "Standard deviation of the F1-score: 0.03\n"
          ]
        }
      ],
      "source": [
        "# removing unused columns to only two features\n",
        "df_LR2 = df_LR7.drop(columns=['sex', 'c_charge_degree', 'c_charge_desc'])\n",
        "\n",
        "# add the total number of previous convictions as a feature \n",
        "df_LR2['total_number'] = df_LR2['juv_fel_count'] + df_LR2['juv_misd_count'] + df_LR2['priors_count']\n",
        "df_LR2 = df_LR2.drop(columns=['juv_fel_count', 'juv_misd_count', 'priors_count'])\n",
        "\n",
        "# splitting the data\n",
        "X_overall_LR2 = df_LR2.drop(columns='two_year_recid')\n",
        "y_overall_LR2 = df_LR2['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_overall_LR2 = cross_val_score(lr, X_overall_LR2, y_overall_LR2, cv=kf, scoring='accuracy')\n",
        "accuracy_overall_LR2 = round(np.average(kfscore_accuracy_overall_LR2) * 100, 2)\n",
        "print('Accuracy overall:', accuracy_overall_LR2)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_overall_LR2_std = round(np.std(kfscore_accuracy_overall_LR2) * 100, 2)\n",
        "table_overall_LR2 = str(accuracy_overall_LR2) + ' [+/-' + str(accuracy_overall_LR2_std) + ']'\n",
        "\n",
        "# F1-score\n",
        "kfscore_f1_overall_LR2 = cross_val_score(lr, X_overall_LR2, y_overall_LR2, cv=kf, scoring='f1')\n",
        "F1_mean_LR2 = round(np.mean(kfscore_f1_overall_LR2), 2)\n",
        "print('Mean of the F1-score:', F1_mean_LR2)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_LR2 = round(np.std(kfscore_f1_overall_LR2), 2)\n",
        "print('Standard deviation of the F1-score:', F1_std_LR2)\n",
        "table_F1_LR2 = str(F1_mean_LR2) + ' [+/-' + str(F1_std_LR2) + ']'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4IiGLpdiF05"
      },
      "source": [
        "#### African-American people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XSb3okRiF06",
        "outputId": "be7c5283-b22b-4988-9633-9fa7363b6862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Black peope: 67.56\n",
            "False Positive for Black people: 35.71\n",
            "False Negative for Black people: 28.93\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# dataset for Black people\n",
        "df_black_LR2 = df_black_LR7.drop(columns=['sex', 'c_charge_degree', 'c_charge_desc'])\n",
        "\n",
        "# add the total number of previous convictions as a feature \n",
        "df_black_LR2['total_number'] = df_black_LR2['juv_fel_count'] + df_black_LR2['juv_misd_count'] + df_black_LR2['priors_count']\n",
        "df_black_LR2 = df_black_LR2.drop(columns=['juv_fel_count', 'juv_misd_count', 'priors_count'])\n",
        "\n",
        "# splitting the data\n",
        "X_black_LR2 = df_black_LR2.drop(columns='two_year_recid')\n",
        "y_black_LR2 = df_black_LR2['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_black_LR2 = cross_val_score(lr, X_black_LR2, y_black_LR2, cv=kf, scoring='accuracy')\n",
        "accuracy_black_LR2 = round(np.average(kfscore_accuracy_black_LR2) * 100, 2)\n",
        "print('Accuracy for Black people:', accuracy_black_LR2)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_black_LR2_std = round(np.std(kfscore_accuracy_black_LR2) * 100, 2)\n",
        "table_black_LR2 = str(accuracy_black_LR2) + ' [+/-' + str(accuracy_black_LR2_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_black_LR2 = cross_val_predict(lr, X_black_LR2, y_black_LR2, cv=10)\n",
        "conf_mat_black_LR2 = confusion_matrix(y_black_LR2, y_pred_black_LR2, normalize='true')\n",
        "FP_black_LR2 = round(conf_mat_black_LR2[0,1] * 100, 2)\n",
        "FN_black_LR2 = round(conf_mat_black_LR2[1,0] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_LR2)\n",
        "print('False Negative for Black people:', FN_black_LR2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sWj0z89iF06"
      },
      "source": [
        "#### Caucasian people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbKe-GxbiF06",
        "outputId": "d25ddcd7-5478-425f-dbe9-adce943d0fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for White people: 67.97\n",
            "False Positive for White people: 10.48\n",
            "False Negative for White people: 65.32\n"
          ]
        }
      ],
      "source": [
        "# dataset for White people\n",
        "df_white_LR2 = df_white_LR7.drop(columns=['sex', 'c_charge_degree', 'c_charge_desc'])\n",
        "\n",
        "# add the total number of previous convictions as a feature \n",
        "df_white_LR2['total_number'] = df_white_LR2['juv_fel_count'] + df_white_LR2['juv_misd_count'] + df_white_LR2['priors_count']\n",
        "df_white_LR2 = df_white_LR2.drop(columns=['juv_fel_count', 'juv_misd_count', 'priors_count'])\n",
        "\n",
        "# splitting the data\n",
        "X_white_LR2 = df_white_LR2.drop(columns='two_year_recid')\n",
        "y_white_LR2 = df_white_LR2['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_white_LR2 = cross_val_score(lr, X_white_LR2, y_white_LR2, cv=kf, scoring='accuracy')\n",
        "accuracy_white_LR2 = round(np.average(kfscore_accuracy_white_LR2) * 100, 2)\n",
        "print('Accuracy for White people:', accuracy_white_LR2)\n",
        "\n",
        "#standard deviation\n",
        "accuracy_white_LR2_std = round(np.std(kfscore_accuracy_overall_LR2) * 100, 2)\n",
        "table_white_LR2 = str(accuracy_white_LR2) + ' [+/-' + str(accuracy_white_LR2_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_white_LR2 = cross_val_predict(lr, X_white_LR2, y_white_LR2, cv=10)\n",
        "conf_mat_white_LR2 = confusion_matrix(y_white_LR2, y_pred_white_LR2, normalize='true')\n",
        "FP_white_LR2 = round(conf_mat_white_LR2[0,1] * 100, 2)\n",
        "FN_white_LR2 = round(conf_mat_white_LR2[1,0] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_LR2)\n",
        "print('False Negative for White people:', FN_white_LR2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JKHiPsZiF06"
      },
      "source": [
        "## Nonlinear Support Vector Machine with radial basis kernel and 7 Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izrRoe9IiF07",
        "outputId": "230f2924-45c1-4b53-a782-860c95c5cea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy overall: 62.84\n",
            "Mean of the F1-score: 0.5\n",
            "Standard deviation of the F1-score: 0.02\n",
            "Accuracy for Black peope: 61.2\n",
            "Accuracy for White people: 61.74\n",
            "False Positive for Black people: 37.77\n",
            "False Negative for Black people: 28.72\n",
            "False Positive for White people: 11.29\n",
            "False Negative for White people: 65.42\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_overall_SVM = cross_val_score(svm, X_overall_LR7, y_overall_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_overall_SVM = round(np.average(kfscore_accuracy_overall_SVM) * 100, 2)\n",
        "print('Accuracy overall:', accuracy_overall_SVM)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_overall_SVM_std = round(np.std(kfscore_accuracy_overall_SVM) * 100, 2)\n",
        "table_overall_SVM = str(accuracy_overall_SVM) + ' [+/-' + str(accuracy_overall_SVM_std) + ']'\n",
        "\n",
        "# F1-score\n",
        "kfscore_f1_overall_SVM = cross_val_score(svm, X_overall_LR7, y_overall_LR7, cv=kf, scoring='f1')\n",
        "F1_mean_SVM = round(np.mean(kfscore_f1_overall_SVM), 2)\n",
        "print('Mean of the F1-score:', F1_mean_SVM)\n",
        "\n",
        "# standard deviaton of the F1-score\n",
        "F1_std_SVM = round(np.std(kfscore_f1_overall_SVM), 2)\n",
        "print('Standard deviation of the F1-score:', F1_std_SVM)\n",
        "table_F1_SVM = str(F1_mean_SVM) + ' [+/-' + str(F1_std_SVM) + ']'\n",
        "\n",
        "# accuracy for Black dataset\n",
        "kfscore_accuracy_black_SVM = cross_val_score(svm, X_black_LR7, y_black_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_black_SVM = round(np.average(kfscore_accuracy_black_SVM) * 100, 2)\n",
        "print('Accuracy for Black people:', accuracy_black_SVM)\n",
        "\n",
        "# standard deviation for Black dataset\n",
        "accuracy_black_SVM_std = round(np.std(kfscore_accuracy_black_SVM) * 100, 2)\n",
        "table_black_SVM = str(accuracy_black_SVM) + ' [+/-' + str(accuracy_black_SVM_std) + ']'\n",
        "\n",
        "# accuracy for White dataset\n",
        "kfscore_accuracy_white_SVM = cross_val_score(svm, X_white_LR7, y_white_LR7, cv=kf, scoring='accuracy')\n",
        "accuracy_white_SVM = round(np.average(kfscore_accuracy_white_SVM) * 100, 2)\n",
        "print('Accuracy for White people:', accuracy_white_SVM)\n",
        "\n",
        "# standard deviation for White dataset\n",
        "accuracy_white_SVM_std = round(np.std(kfscore_accuracy_white_SVM) * 100, 2)\n",
        "table_white_SVM = str(accuracy_white_SVM) + ' [+/-' + str(accuracy_white_SVM_std) + ']'\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_SVM = cross_val_predict(svm, X_black_LR7, y_black_LR7, cv=kf)\n",
        "conf_mat_black_SVM = confusion_matrix(y_black_LR7, y_pred_black_LR7, normalize='true')\n",
        "FP_black_SVM = round(conf_mat_black_SVM[0,1] * 100, 2)\n",
        "FN_black_SVM = round(conf_mat_black_SVM[1,0] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_SVM)\n",
        "print('False Negative for Black people:', FN_black_SVM)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_SVM = cross_val_predict(svm, X_white_LR7, y_white_LR7, cv=kf)\n",
        "conf_mat_white_SVM = confusion_matrix(y_white_LR7, y_pred_white_LR7, normalize='true')\n",
        "FP_white_SVM = round(conf_mat_white_SVM[0,1] * 100, 2)\n",
        "FN_white_SVM = round(conf_mat_white_SVM[1,0] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_SVM)\n",
        "print('False Negative for White people:', FN_white_SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfMyYlRWiF07"
      },
      "source": [
        "## Logistic Regression with 8 features (including race)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbdF81IeiF07"
      },
      "source": [
        "#### Overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xszu2qvniF07",
        "outputId": "ccfe5446-c216-470f-d56d-e7d7e27c73f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy overall: 67.74\n",
            "Mean of the F1-score: 0.6\n",
            "Standard deviation of the F1-score: 0.02\n"
          ]
        }
      ],
      "source": [
        "# removing unused columns\n",
        "df_LR8 = df_compas.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# encoding\n",
        "df_LR8['sex'] = df_LR8['sex'].astype('category')\n",
        "df_LR8['sex'] = df_LR8['sex'].cat.codes\n",
        "df_LR8['c_charge_degree'] = df_LR8['c_charge_degree'].astype('category')\n",
        "df_LR8['c_charge_degree'] = df_LR8['c_charge_degree'].cat.codes\n",
        "df_LR8['c_charge_desc'] = df_LR8['c_charge_desc'].astype('category')\n",
        "df_LR8['c_charge_desc'] = df_LR8['c_charge_desc'].cat.codes\n",
        "df_LR8['race'] = df_LR8['race'].astype('category')\n",
        "df_LR8['race'] = df_LR8['race'].cat.codes\n",
        "\n",
        "# splitting the data\n",
        "X_overall_LR8 = df_LR8.drop(columns='two_year_recid')\n",
        "y_overall_LR8 = df_LR8['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_overall_LR8 = cross_val_score(lr, X_overall_LR8, y_overall_LR8, cv=kf, scoring='accuracy')\n",
        "accuracy_overall_LR8 = round(np.average(kfscore_accuracy_overall_LR8) * 100, 2)\n",
        "print('Accuracy overall:', accuracy_overall_LR8)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_overall_LR8_std = round(np.std(kfscore_accuracy_overall_LR8) * 100, 2)\n",
        "table_overall_LR8 = str(accuracy_overall_LR8) + ' [+/-' + str(accuracy_overall_LR8_std) + ']'\n",
        "\n",
        "# F1-score\n",
        "kfscore_f1_overall_LR8 = cross_val_score(lr, X_overall_LR8, y_overall_LR8, cv=kf, scoring='f1')\n",
        "F1_mean_LR8 = round(np.mean(kfscore_f1_overall_LR8), 2)\n",
        "print('Mean of the F1-score:', F1_mean_LR8)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_LR8 = round(np.std(kfscore_f1_overall_LR8), 2)\n",
        "print('Standard deviation of the F1-score:', F1_std_LR8)\n",
        "table_F1_LR8 = str(F1_mean_LR8) + ' [+/-' + str(F1_std_LR8) + ']'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JE0zi2JiF08"
      },
      "source": [
        "#### African-American people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PkQtk5IiF08",
        "outputId": "5e521ba5-db99-494b-ec44-d2b202afb672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Black peope: 66.91\n",
            "False Positive for Black people: 37.77\n",
            "False Negative for Black people: 28.67\n"
          ]
        }
      ],
      "source": [
        "# dataset for Black people\n",
        "df_black_LR8 = df_compas.loc[df_compas['race'] == \"African-American\"]\n",
        "df_black_LR8 = df_black_LR8.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# encoding\n",
        "df_black_LR8['sex'] = df_black_LR8['sex'].astype('category')\n",
        "df_black_LR8['sex'] = df_black_LR8['sex'].cat.codes\n",
        "df_black_LR8['c_charge_degree'] = df_black_LR8['c_charge_degree'].astype('category')\n",
        "df_black_LR8['c_charge_degree'] = df_black_LR8['c_charge_degree'].cat.codes\n",
        "df_black_LR8['c_charge_desc'] = df_black_LR8['c_charge_desc'].astype('category')\n",
        "df_black_LR8['c_charge_desc'] = df_black_LR8['c_charge_desc'].cat.codes\n",
        "df_black_LR8['race'] = df_black_LR8['race'].astype('category')\n",
        "df_black_LR8['race'] = df_black_LR8['race'].cat.codes\n",
        "\n",
        "# splitting the data\n",
        "X_black_LR8 = df_black_LR8.drop(columns='two_year_recid')\n",
        "y_black_LR8 = df_black_LR8['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_black_LR8 = cross_val_score(lr, X_black_LR8, y_black_LR8, cv=kf, scoring='accuracy')\n",
        "accuracy_black_LR8 = round(np.average(kfscore_accuracy_black_LR8) * 100, 2)\n",
        "print('Accuracy for Black people:', accuracy_black_LR8)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_black_LR8_std = round(np.std(kfscore_accuracy_black_LR8) * 100, 2)\n",
        "table_black_LR8 = str(accuracy_black_LR8) + ' [+/-' + str(accuracy_black_LR8_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_black_LR8 = cross_val_predict(lr, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_LR8 = confusion_matrix(y_black_LR8, y_pred_black_LR8, normalize='true')\n",
        "FP_black_LR8 = round(conf_mat_black_LR8[0,1] * 100, 2)\n",
        "FN_black_LR8 = round(conf_mat_black_LR8[1,0] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_LR8)\n",
        "print('False Negative for Black people:', FN_black_LR8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izempWdUiF08"
      },
      "source": [
        "#### Caucasian people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baoH4EmTiF08",
        "outputId": "052f66da-25ed-4000-bf0e-1f31ff201181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for White people: 67.4\n",
            "False Positive for White people: 11.29\n",
            "False Negative for White people: 65.42\n"
          ]
        }
      ],
      "source": [
        "# dataset for White people\n",
        "df_white_LR8 = df_compas.loc[df_compas['race'] == 'Caucasian']\n",
        "df_white_LR8 = df_white_LR8.drop(columns=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob',\n",
        "       'age_cat', 'decile_score', 'juv_other_count',\n",
        "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
        "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'is_recid', 'r_case_number',\n",
        "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
        "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
        "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
        "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
        "       'decile_score.1', 'score_text', 'screening_date',\n",
        "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
        "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
        "       'start', 'end', 'event'])\n",
        "\n",
        "# encoding\n",
        "df_white_LR8['sex'] = df_white_LR8['sex'].astype('category')\n",
        "df_white_LR8['sex'] = df_white_LR8['sex'].cat.codes\n",
        "df_white_LR8['c_charge_degree'] = df_white_LR8['c_charge_degree'].astype('category')\n",
        "df_white_LR8['c_charge_degree'] = df_white_LR8['c_charge_degree'].cat.codes\n",
        "df_white_LR8['c_charge_desc'] = df_white_LR8['c_charge_desc'].astype('category')\n",
        "df_white_LR8['c_charge_desc'] = df_white_LR8['c_charge_desc'].cat.codes\n",
        "df_white_LR8['race'] = df_white_LR8['race'].astype('category')\n",
        "df_white_LR8['race'] = df_white_LR8['race'].cat.codes\n",
        "\n",
        "# splitting the data\n",
        "X_white_LR8 = df_white_LR8.drop(columns='two_year_recid')\n",
        "y_white_LR8 = df_white_LR8['two_year_recid']\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_white_LR8 = cross_val_score(lr, X_white_LR8, y_white_LR8, cv=kf, scoring='accuracy')\n",
        "accuracy_white_LR8 = round(np.average(kfscore_accuracy_white_LR8) * 100, 2)\n",
        "print('Accuracy for White people:', accuracy_white_LR8)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_white_LR8_std = round(np.std(kfscore_accuracy_overall_LR8) * 100, 2)\n",
        "table_white_LR8 = str(accuracy_overall_LR8) + ' [+/-' + str(accuracy_overall_LR8_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_white_LR8 = cross_val_predict(lr, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_LR8 = confusion_matrix(y_white_LR8, y_pred_white_LR8, normalize='true')\n",
        "FP_white_LR8 = round(conf_mat_white_LR8[0,1] * 100, 2)\n",
        "FN_white_LR8 = round(conf_mat_white_LR8[1,0] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_LR8)\n",
        "print('False Negative for White people:', FN_white_LR8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q27l7iXEiF09"
      },
      "source": [
        "## Logistic Regression with all features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcr4pTLhiF09"
      },
      "source": [
        "#### Overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0Rnc8tdtiF09"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "\n",
        "# splitting the data\n",
        "X_overall_LR = df_compas.drop(columns='two_year_recid')\n",
        "y_overall_LR = df_compas['two_year_recid']\n",
        "\n",
        "# sort data into numerical and categorical columns\n",
        "numerical_cols = X_overall_LR.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = X_overall_LR.select_dtypes(include=['object']).columns\n",
        "\n",
        "# pipeline to encode the data\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0.0)),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='-1')),\n",
        "    ('enocder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# preprocessor to combine the transformers\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ").set_output(transform='pandas')\n",
        "\n",
        "# add the preprocessor to the model\n",
        "lr_process = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', lr)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Buq_Z0jWiF09",
        "outputId": "1624c53e-4fb1-4a89-bc1a-c0697fab372a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy overall: 97.6\n",
            "Mean of the F1-score: 0.97\n",
            "Standard deviation of the F1-score: 0.01\n"
          ]
        }
      ],
      "source": [
        "# accuracy\n",
        "kfscore_accuracy_overall_LR = cross_val_score(lr_process, X_overall_LR, y_overall_LR, cv=kf, scoring='accuracy')\n",
        "accuracy_overall_LR = round(np.average(kfscore_accuracy_overall_LR) * 100, 2)\n",
        "print('Accuracy overall:', accuracy_overall_LR)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_overall_LR_std = round(np.std(kfscore_accuracy_overall_LR) * 100, 2)\n",
        "table_overall_LR = str(accuracy_overall_LR) + ' [+/-' + str(accuracy_overall_LR_std) + ']'\n",
        "\n",
        "# F1-score\n",
        "kfscore_f1_overall_LR = cross_val_score(lr_process, X_overall_LR, y_overall_LR, cv=kf, scoring='f1')\n",
        "F1_mean_LR = round(np.mean(kfscore_f1_overall_LR), 2)\n",
        "print('Mean of the F1-score:', F1_mean_LR)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_LR = round(np.std(kfscore_f1_overall_LR), 2)\n",
        "print('Standard deviation of the F1-score:', F1_std_LR)\n",
        "table_F1_LR = str(F1_mean_LR) + ' [+/-' + str(F1_std_LR) + ']'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mw4gVpriF0-"
      },
      "source": [
        "#### African-American people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w42aHbcbiF0-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Black people: 96.78\n",
            "False Positive for Black people: 6.35\n",
            "False Negative for Black people: 0.26\n"
          ]
        }
      ],
      "source": [
        "# dataset for Black people\n",
        "df_black_LR = df_compas.loc[df_compas['race'] == \"African-American\"]\n",
        "\n",
        "# splitting the data\n",
        "X_black_LR = df_black_LR.drop(columns='two_year_recid')\n",
        "y_black_LR = df_black_LR['two_year_recid']\n",
        "\n",
        "# sort data into numerical and categorical columns\n",
        "numerical_cols = X_black_LR.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = X_black_LR.select_dtypes(include=['object']).columns\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_black_LR = cross_val_score(lr_process, X_black_LR, y_black_LR, cv=kf, scoring='accuracy')\n",
        "accuracy_black_LR = round(np.average(kfscore_accuracy_black_LR) * 100, 2)\n",
        "print('Accuracy for Black people:', accuracy_black_LR)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_black_LR_std = round(np.std(kfscore_accuracy_black_LR) * 100, 2)\n",
        "table_black_LR = str(accuracy_black_LR) + ' [+/-' + str(accuracy_black_LR_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_black_LR = cross_val_predict(lr_process, X_black_LR, y_black_LR, cv=kf)\n",
        "conf_mat_black_LR = confusion_matrix(y_black_LR, y_pred_black_LR, normalize='true')\n",
        "FP_black_LR = round(conf_mat_black_LR[0,1] * 100, 2)\n",
        "FN_black_LR = round(conf_mat_black_LR[1,0] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_LR)\n",
        "print('False Negative for Black people:',  FN_black_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ehPxPMGiF0-"
      },
      "source": [
        "#### Caucasian people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2SUwnQcfiF0-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for White people: 97.68\n",
            "False Positive for White people: 3.83\n",
            "False Negative for White people: 0.0\n"
          ]
        }
      ],
      "source": [
        "# dataset for White people\n",
        "df_white_LR = df_compas.loc[df_compas['race'] == 'Caucasian']\n",
        "\n",
        "# splitting the data\n",
        "X_white_LR = df_white_LR.drop(columns='two_year_recid')\n",
        "y_white_LR = df_white_LR['two_year_recid']\n",
        "\n",
        "# sort data into numerical and categorical columns\n",
        "numerical_cols = X_white_LR.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = X_white_LR.select_dtypes(include=['object']).columns\n",
        "\n",
        "# accuracy\n",
        "kfscore_accuracy_white_LR = cross_val_score(lr_process, X_white_LR, y_white_LR, cv=kf, scoring='accuracy')\n",
        "accuracy_white_LR = round(np.average(kfscore_accuracy_white_LR) * 100, 2)\n",
        "print('Accuracy for White people:', accuracy_white_LR)\n",
        "\n",
        "# standard deviation\n",
        "accuracy_white_LR_std = round(np.std(kfscore_accuracy_overall_LR) * 100, 2)\n",
        "table_white_LR = str(accuracy_overall_LR) + ' [+/-' + str(accuracy_overall_LR_std) + ']'\n",
        "\n",
        "# false positive and false negative\n",
        "y_pred_white_LR = cross_val_predict(lr_process, X_white_LR, y_white_LR, cv=kf)\n",
        "conf_mat_white_LR = confusion_matrix(y_white_LR, y_pred_white_LR, normalize='true')\n",
        "FP_white_LR = round(conf_mat_white_LR[0,1] * 100, 2)\n",
        "FN_white_LR = round(conf_mat_white_LR[1,0] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_LR)\n",
        "print('False Negative for White people:', FN_white_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wtawusdiF0_"
      },
      "source": [
        "####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNb7ExPIiF1A"
      },
      "source": [
        "## Recreation of Table 2 in Dressel and Farids (2018) paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Et4ShHvHiF1B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Metric                 â”‚ LR7             â”‚ LR2             â”‚ NL-SVM          â”‚ COMPAS   â”‚ LR8             â”‚ LR              â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Accuracy (overall)     â”‚ 67.58 [+/-1.62] â”‚ 67.51 [+/-2.14] â”‚ 62.84 [+/-1.4]  â”‚ 65.4     â”‚ 67.74 [+/-1.88] â”‚ 97.6 [+/-0.64]  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Accuracy (black)       â”‚ 66.88 [+/-1.81] â”‚ 67.56 [+/-1.62] â”‚ 61.2 [+/-3.31]  â”‚ 63.8     â”‚ 66.91 [+/-1.87] â”‚ 96.78 [+/-0.61] â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Accuracy (white)       â”‚ 67.58 [+/-1.62] â”‚ 67.97 [+/-2.14] â”‚ 61.74 [+/-0.95] â”‚ 67.0     â”‚ 67.74 [+/-1.88] â”‚ 97.6 [+/-0.64]  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ False Positive (black) â”‚ 37.77           â”‚ 35.71           â”‚ 37.77           â”‚ 44.8     â”‚ 37.77           â”‚ 6.35            â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ False Positive (white) â”‚ 11.29           â”‚ 10.48           â”‚ 11.29           â”‚ 23.5     â”‚ 11.29           â”‚ 3.83            â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ False Negative (black) â”‚ 28.72           â”‚ 28.93           â”‚ 28.72           â”‚ 28.0     â”‚ 28.67           â”‚ 0.26            â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ False Negative (white) â”‚ 65.42           â”‚ 65.32           â”‚ 65.42           â”‚ 47.7     â”‚ 65.42           â”‚ 0.0             â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Mean (F1)              â”‚ 0.6 [+/-0.02]   â”‚ 0.58 [+/-0.03]  â”‚ 0.5 [+/-0.02]   â”‚ -        â”‚ 0.6 [+/-0.02]   â”‚ 0.97 [+/-0.01]  â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "header = [\"Metric\", \"LR7\", \"LR2\", \"NL-SVM\", \"COMPAS\", \"LR8\", \"LR\"]\n",
        "\n",
        "data = [\n",
        "    [\"Accuracy (overall)\", table_overall_LR7, table_overall_LR2, table_overall_SVM, \"65.4\", table_overall_LR8, table_overall_LR],\n",
        "    [\"Accuracy (black)\", table_black_LR7, table_black_LR2, table_black_SVM, \"63.8\", table_black_LR8, table_black_LR],\n",
        "    [\"Accuracy (white)\", table_white_LR7, table_white_LR2, table_white_SVM, \"67.0\", table_white_LR8, table_white_LR],\n",
        "    [\"False Positive (black)\", FP_black_LR7, FP_black_LR2, FP_black_SVM, \"44.8\", FP_black_LR8, FP_black_LR],\n",
        "    [\"False Positive (white)\", FP_white_LR7, FP_white_LR2, FP_white_SVM, \"23.5\", FP_white_LR8, FP_white_LR],\n",
        "    [\"False Negative (black)\", FN_black_LR7, FN_black_LR2, FN_black_SVM, \"28.0\", FN_black_LR8, FN_black_LR],\n",
        "    [\"False Negative (white)\", FN_white_LR7, FN_white_LR2, FN_white_SVM, \"47.7\", FN_white_LR8, FN_white_LR],\n",
        "    [\"Mean (F1)\", table_F1_LR7, table_F1_LR2, table_F1_SVM, \"-\", table_F1_LR8, table_F1_LR ],\n",
        "]\n",
        "\n",
        "print(tabulate(data, headers=header, tablefmt=\"fancy_grid\", floatfmt=\".2f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pDcayG3iF1B"
      },
      "source": [
        "# Mitigation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBAxb_QbiF1B"
      },
      "source": [
        "#### Vanilla XG Boost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEiiVoeFiF1B",
        "outputId": "e5b46a5a-2d08-40e5-c191-75f39f6c28cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6005\n",
            "Standard deviation of the F1-score: 0.0184\n",
            "False Positive for Black people: 34.15\n",
            "False Negative for Black people: 34.67\n",
            "False Positive for White people: 24.73\n",
            "False Negative for White people: 54.24\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "# the chosen parameters originate from the paper about Tree Boosting Methods\n",
        "\n",
        "# parameters \n",
        "params_vanilla = {\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.3,\n",
        "    'subsample': 1,\n",
        "    'colsample_bytree': 1,\n",
        "}\n",
        "\n",
        "# model\n",
        "xgb_vanilla = XGBClassifier(**params_vanilla)\n",
        "\n",
        "# F1-score\n",
        "xgb_cv = cross_val_score(xgb_vanilla, X_overall_LR8, y_overall_LR8, cv=kf, scoring='f1')\n",
        "F1_mean_vanilla = round(np.mean(xgb_cv), 4)\n",
        "print('Mean of the F1-score:', F1_mean_vanilla)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_vanilla = round(np.std(xgb_cv), 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_vanilla)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_vanilla = cross_val_predict(xgb_vanilla, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_vanilla = confusion_matrix(y_black_LR8, y_pred_black_vanilla, normalize='true')\n",
        "FP_black_vanilla = round(conf_mat_black_vanilla[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_vanilla)\n",
        "FN_black_vanilla = round(conf_mat_black_vanilla[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_vanilla)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_vanilla = cross_val_predict(xgb_vanilla, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_vanilla = confusion_matrix(y_white_LR8, y_pred_white_vanilla, normalize='true')\n",
        "FP_white_vanilla = round(conf_mat_white_vanilla[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_vanilla)\n",
        "FN_white_vanilla = round(conf_mat_white_vanilla[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_vanilla)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN6R3pHliF1C"
      },
      "source": [
        "#### XG Boost with RandomizedSearch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uBk-VUuiF1C",
        "outputId": "714bb725-b43e-46f4-d4f3-112812d792ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6391\n",
            "Standard deviation of the F1-score: 0.0266\n",
            "False Positive for Black people: 33.15\n",
            "False Negative for Black people: 31.51\n",
            "False Positive for White people: 15.52\n",
            "False Negative for White people: 57.56\n"
          ]
        }
      ],
      "source": [
        "# beginning parameters for tuning are from the paper about Tree Boosting Methods\n",
        "# param = {\n",
        "#     'max_depth': [3, 6, 12, 20],\n",
        "#     'learning_rate': [0.02, 0.1, 0.2],\n",
        "#     'subsample': [0.4, 0.8, 1],\n",
        "#     'colsample_bytree': [0.4, 0.6, 1],\n",
        "#     'n_estimators': [100, 1000, 5000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_rs = {\n",
        "    'max_depth': [3],\n",
        "    'learning_rate': [0.1],\n",
        "    'subsample': [1],\n",
        "    'colsample_bytree': [0.6],\n",
        "    'n_estimators': [100],\n",
        "}\n",
        "\n",
        "# model\n",
        "xgb_rs = XGBClassifier()\n",
        "\n",
        "# randomized search and tuning\n",
        "random_search = RandomizedSearchCV(xgb_rs, param_distributions=param_rs, scoring='f1', cv=kf, n_iter=1)\n",
        "random_search.fit(X_overall_LR8, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_rs = round(np.mean(random_search.score(X_overall_LR8, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_rs)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_rs = random_search.cv_results_['std_test_score']\n",
        "F1_std_rs_rounded = round(F1_std_rs[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_rs_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_rs = cross_val_predict(random_search, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_rs = confusion_matrix(y_black_LR8, y_pred_black_rs, normalize='true')\n",
        "FP_black_rs = round(conf_mat_black_rs[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_rs)\n",
        "FN_black_rs = round(conf_mat_black_rs[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_rs)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_rs = cross_val_predict(random_search, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_rs = confusion_matrix(y_white_LR8, y_pred_white_rs, normalize='true')\n",
        "FP_white_rs = round(conf_mat_white_rs[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_rs)\n",
        "FN_white_rs = round(conf_mat_white_rs[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-6nBLRiF1D"
      },
      "source": [
        "#### XG Boost with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6JFTmA0iF1D",
        "outputId": "2b735a98-63ac-46eb-84c2-28a962d9eadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6464\n",
            "Standard deviation of the F1-score: 0.0258\n",
            "False Positive for Black people: 33.15\n",
            "False Negative for Black people: 31.51\n",
            "False Positive for White people: 15.52\n",
            "False Negative for White people: 57.56\n"
          ]
        }
      ],
      "source": [
        "# beginning parameters for tuning are from the paper about Tree Boosting Methods\n",
        "# param = {\n",
        "#     'max_depth': [3, 6, 12, 20],\n",
        "#     'learning_rate': [0.02, 0.1, 0.2],\n",
        "#     'subsample': [0.4, 0.8, 1],\n",
        "#     'colsample_bytree': [0.4, 0.6, 1],\n",
        "#     'n_estimators': [100, 1000, 5000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_gs = {\n",
        "    'max_depth': [3],\n",
        "    'learning_rate': [0.1],\n",
        "    'subsample': [0.4],\n",
        "    'colsample_bytree': [0.6],\n",
        "    'n_estimators': [100],\n",
        "}\n",
        "\n",
        "# model\n",
        "xgb_gs = XGBClassifier()\n",
        "\n",
        "# grid search and tuning\n",
        "grid_search = GridSearchCV(xgb_gs, param_grid=param_gs, scoring='f1', cv=kf, n_jobs=-1)\n",
        "grid_search.fit(X_overall_LR8, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_gs = round(np.mean(grid_search.score(X_overall_LR8, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_gs)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_gs = grid_search.cv_results_['std_test_score']\n",
        "F1_std_gs_rounded = round(F1_std_gs[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_gs_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_gs = cross_val_predict(grid_search, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_gs = confusion_matrix(y_black_LR8, y_pred_black_gs, normalize='true')\n",
        "FP_black_gs = round(conf_mat_black_gs[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_rs)\n",
        "FN_black_gs = round(conf_mat_black_gs[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_rs)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_gs = cross_val_predict(grid_search, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_gs = confusion_matrix(y_white_LR8, y_pred_white_gs, normalize='true')\n",
        "FP_white_gs = round(conf_mat_white_gs[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_rs)\n",
        "FN_white_gs = round(conf_mat_white_gs[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_rs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKPcxjGriF1D"
      },
      "source": [
        "#### Grid Search with XG Boost and Fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0710--OiF1D",
        "outputId": "90a563cb-69d5-45fc-90f1-36f9a48bbcc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6495\n",
            "Standard deviation of the F1-score: 0.0277\n",
            "False Positive for Black people: 34.09\n",
            "False Negative for Black people: 30.56\n",
            "False Positive for White people: 17.88\n",
            "False Negative for White people: 56.73\n"
          ]
        }
      ],
      "source": [
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "\n",
        "# transforming the data with the correlation remover\n",
        "cr = CorrelationRemover(sensitive_feature_ids=['race'])\n",
        "X_overall_fairlearn = cr.fit_transform(X_overall_LR8)\n",
        "\n",
        "# beginning parameters for tuning are from the paper about Tree Boosting Methods\n",
        "# param = {\n",
        "#     'max_depth': [3, 6, 12, 20],\n",
        "#     'learning_rate': [0.02, 0.1, 0.2],\n",
        "#     'subsample': [0.4, 0.8, 1],\n",
        "#     'colsample_bytree': [0.4, 0.6, 1],\n",
        "#     'n_estimators': [100, 1000, 5000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_gs_cr = {\n",
        "    'max_depth': [3],\n",
        "    'learning_rate': [0.2],\n",
        "    'subsample': [0.4],\n",
        "    'colsample_bytree': [0.4],\n",
        "    'n_estimators': [100],\n",
        "}\n",
        "\n",
        "# model\n",
        "xgb_gs_cr = XGBClassifier()\n",
        "\n",
        "# grid search and tuning\n",
        "grid_search_cr = GridSearchCV(xgb_gs_cr, param_grid=param_gs_cr, scoring='f1', cv=kf, n_jobs=-1)\n",
        "grid_search_cr.fit(X_overall_fairlearn, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_gs_cr = round(np.mean(grid_search_cr.score(X_overall_fairlearn, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_gs_cr)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_gs_cr = grid_search_cr.cv_results_['std_test_score']\n",
        "F1_std_gs_cr_rounded = round(F1_std_gs_cr[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_gs_cr_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "X_black_fairlearn = cr.fit_transform(X_black_LR8)\n",
        "y_pred_black_gs_cr = cross_val_predict(grid_search_cr, X_black_fairlearn, y_black_LR8, cv=kf)\n",
        "conf_mat_black_gs_cr = confusion_matrix(y_black_LR8, y_pred_black_gs_cr, normalize='true')\n",
        "FP_black_gs_cr = round(conf_mat_black_gs_cr[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_gs_cr)\n",
        "FN_black_gs_cr = round(conf_mat_black_gs_cr[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_gs_cr)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "X_white_fairlearn = cr.fit_transform(X_white_LR8)\n",
        "y_pred_white_gs_cr = cross_val_predict(grid_search_cr, X_white_fairlearn, y_white_LR8, cv=kf)\n",
        "conf_mat_white_gs_cr = confusion_matrix(y_white_LR8, y_pred_white_gs_cr, normalize='true')\n",
        "FP_white_gs_cr = round(conf_mat_white_gs_cr[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_gs_cr)\n",
        "FN_white_gs_cr = round(conf_mat_white_gs_cr[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_gs_cr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckUkz8rF4DCS"
      },
      "source": [
        "#### Logistic Regression with RandomizedSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1BbONsj1an6",
        "outputId": "149e59c0-6a62-48cb-fdef-9a7a099ca025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6033\n",
            "Standard deviation of the F1-score: 0.0208\n",
            "False Positive for Black people: 37.72\n",
            "False Negative for Black people: 28.67\n",
            "False Positive for White people: 11.29\n",
            "False Negative for White people: 65.42\n"
          ]
        }
      ],
      "source": [
        "# beginning parameters for tuning are from the articles in Level Up Coding, Toward Data Science and StackOverflow\n",
        "# the default solver of the logistic regression is 'lbfgs'. In order to compare it to the previous results with LR this is not changed.\n",
        "# param = {\n",
        "#     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "#     'penalty': ['l2', None],\n",
        "#     'solver': ['lbfgs'], \n",
        "#     'max_iter': [1000, 1500, 2000, 2500, 3000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_lr_rs = {\n",
        "    'C': [100],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'], # the default solver of the logistic regression is 'lbfgs'. In order to compare it to the previous results with LR this is not changed.\n",
        "    'max_iter': [1000],\n",
        "}\n",
        "\n",
        "# model\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# random search and tuning\n",
        "random_search_lr = RandomizedSearchCV(lr, param_distributions=param_lr_rs, scoring='f1', cv=kf, n_iter=1)\n",
        "random_search_lr.fit(X_overall_LR8, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_lr_rs = round(np.mean(random_search_lr.score(X_overall_LR8, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_lr_rs)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_lr_rs = random_search_lr.cv_results_['std_test_score']\n",
        "F1_std_lr_rs_rounded = round(F1_std_lr_rs[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_lr_rs_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_lr_rs = cross_val_predict(random_search_lr, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_lr_rs = confusion_matrix(y_black_LR8, y_pred_black_lr_rs, normalize='true')\n",
        "FP_black_lr_rs = round(conf_mat_black_lr_rs[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_lr_rs)\n",
        "FN_black_lr_rs = round(conf_mat_black_lr_rs[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_lr_rs)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_lr_rs = cross_val_predict(random_search_lr, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_lr_rs = confusion_matrix(y_white_LR8, y_pred_white_lr_rs, normalize='true')\n",
        "FP_white_lr_rs = round(conf_mat_white_lr_rs[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_lr_rs)\n",
        "FN_white_lr_rs = round(conf_mat_white_lr_rs[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_lr_rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWLnQEM_4MhO"
      },
      "source": [
        "#### Logistic Regression with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4xo1wctY9H-",
        "outputId": "a7065f4e-3772-4da9-d35b-f51f6dd9fa28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.6033\n",
            "Standard deviation of the F1-score: 0.0209\n",
            "False Positive for Black people: 37.77\n",
            "False Negative for Black people: 28.67\n",
            "False Positive for White people: 11.29\n",
            "False Negative for White people: 65.42\n"
          ]
        }
      ],
      "source": [
        "# beginning parameters for tuning are from the articles in Level Up Coding, Toward Data Science and StackOverflow\n",
        "# the default solver of the logistic regression is 'lbfgs'. In order to compare it to the previous results with LR this is not changed.\n",
        "# param = {\n",
        "#     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "#     'penalty': ['l2', None],\n",
        "#     'solver': ['lbfgs'], \n",
        "#     'max_iter': [1000, 1500, 2000, 2500, 3000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_lr_gs = {\n",
        "    'C': [1000],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'], # the default solver of the logistic regression is 'lbfgs'. In order to compare it to the previous results with LR this is not changed.\n",
        "    'max_iter': [1000],\n",
        "}\n",
        "\n",
        "# model\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# grid search and tuning\n",
        "grid_search_lr = GridSearchCV(lr, param_grid=param_lr_gs, scoring='f1', cv=kf)\n",
        "grid_search_lr.fit(X_overall_LR8, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_lr_gs = round(np.mean(grid_search_lr.score(X_overall_LR8, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_lr_gs)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_lr_gs = grid_search_lr.cv_results_['std_test_score']\n",
        "F1_std_lr_gs_rounded = round(F1_std_lr_gs[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_lr_gs_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "y_pred_black_lr_gs = cross_val_predict(grid_search_lr, X_black_LR8, y_black_LR8, cv=kf)\n",
        "conf_mat_black_lr_gs = confusion_matrix(y_black_LR8, y_pred_black_lr_gs, normalize='true')\n",
        "FP_black_lr_gs = round(conf_mat_black_lr_gs[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_lr_gs)\n",
        "FN_black_lr_gs = round(conf_mat_black_lr_gs[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_lr_gs)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "y_pred_white_lr_gs = cross_val_predict(grid_search_lr, X_white_LR8, y_white_LR8, cv=kf)\n",
        "conf_mat_white_lr_gs = confusion_matrix(y_white_LR8, y_pred_white_lr_gs, normalize='true')\n",
        "FP_white_lr_gs = round(conf_mat_white_lr_gs[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_lr_gs)\n",
        "FN_white_lr_gs = round(conf_mat_white_lr_gs[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_lr_gs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cHTPRzhtNBK"
      },
      "source": [
        "#### Grid Search with Logistic Regression and Fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVbc7V-ltNfJ",
        "outputId": "0dbf250c-4bc8-4652-a4f2-58683b520169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the F1-score: 0.5721\n",
            "Standard deviation of the F1-score: 0.0282\n",
            "False Positive for Black people: 37.66\n",
            "False Negative for Black people: 28.88\n",
            "False Positive for White people: 11.42\n",
            "False Negative for White people: 65.53\n"
          ]
        }
      ],
      "source": [
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "\n",
        "# transforming the data with the correlation remover\n",
        "cr = CorrelationRemover(sensitive_feature_ids=['race'])\n",
        "X_overall_fairlearn = cr.fit_transform(X_overall_LR8)\n",
        "\n",
        "# beginning parameters for tuning are from the articles in Level Up Coding, Toward Data Science and StackOverflow\n",
        "# the default solver of the logistic regression is 'lbfgs'. In order to compare it to the previous results with LR this is not changed.\n",
        "# param = {\n",
        "#     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "#     'penalty': ['l2', None],\n",
        "#     'solver': ['lbfgs'], \n",
        "#     'max_iter': [1000, 1500, 2000, 2500, 3000],\n",
        "# }\n",
        "\n",
        "# parameters\n",
        "param_lr_gs_cr = {\n",
        "    'C': [0.1],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'],\n",
        "    'max_iter': [1000],\n",
        "}\n",
        "\n",
        "# model\n",
        "lr_gs_cr = LogisticRegression(**param_lr_gs_cr)\n",
        "\n",
        "# grid search and tuning\n",
        "grid_search_lr_cr = GridSearchCV(lr_gs_cr, param_grid=param_lr_gs_cr, scoring='f1', cv=kf)\n",
        "grid_search_lr_cr.fit(X_overall_fairlearn, y_overall_LR8)\n",
        "\n",
        "# F1-score\n",
        "F1_mean_lr_gs_cr = round(np.mean(grid_search_lr_cr.score(X_overall_fairlearn, y_overall_LR8)), 4)\n",
        "print('Mean of the F1-score:', F1_mean_lr_gs_cr)\n",
        "\n",
        "# standard deviation of the F1-score\n",
        "F1_std_lr_gs_cr = grid_search_lr_cr.cv_results_['std_test_score']\n",
        "F1_std_lr_gs_cr_rounded = round(F1_std_lr_gs_cr[0], 4)\n",
        "print('Standard deviation of the F1-score:', F1_std_lr_gs_cr_rounded)\n",
        "\n",
        "# false positive and false negative for Black dataset\n",
        "X_black_lr_fairlearn = cr.fit_transform(X_black_LR8)\n",
        "y_pred_black_lr_gs_cr = cross_val_predict(grid_search_lr_cr, X_black_lr_fairlearn, y_black_LR8, cv=kf)\n",
        "conf_mat_black_lr_gs_cr = confusion_matrix(y_black_LR8, y_pred_black_lr_gs_cr, normalize='true')\n",
        "FP_black_lr_gs_cr = round(conf_mat_black_lr_gs_cr[0,1] * 100, 2)\n",
        "print('False Positive for Black people:', FP_black_lr_gs_cr)\n",
        "FN_black_lr_gs_cr = round(conf_mat_black_lr_gs_cr[1,0] * 100, 2)\n",
        "print('False Negative for Black people:', FN_black_lr_gs_cr)\n",
        "\n",
        "# false positive and false negative for White dataset\n",
        "X_white_lr_fairlearn = cr.fit_transform(X_white_LR8)\n",
        "y_pred_white_lr_gs_cr = cross_val_predict(grid_search_lr_cr, X_white_lr_fairlearn, y_white_LR8, cv=kf)\n",
        "conf_mat_white_lr_gs_cr = confusion_matrix(y_white_LR8, y_pred_white_lr_gs_cr, normalize='true')\n",
        "FP_white_lr_gs_cr = round(conf_mat_white_lr_gs_cr[0,1] * 100, 2)\n",
        "print('False Positive for White people:', FP_white_lr_gs_cr)\n",
        "FN_white_lr_gs_cr = round(conf_mat_white_lr_gs_cr[1,0] * 100, 2)\n",
        "print('False Negative for White people:', FN_white_lr_gs_cr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k50I809kzVwU",
        "outputId": "72a51f76-b0f3-4a30-d412-eb657ae2040b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                     â”‚                         â”‚   Vanilla â”‚   Randomized Search â”‚   Grid Search â”‚   Fairlearn CorrelationRemover â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ XG-Boost            â”‚ F1-score                â”‚    0.6005 â”‚              0.6391 â”‚        0.6464 â”‚                         0.6495 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ F1-standard deviation   â”‚    0.0184 â”‚              0.0266 â”‚        0.0258 â”‚                         0.0277 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Positives - White â”‚   24.7300 â”‚             15.5200 â”‚       16.8000 â”‚                        17.8800 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Positives - Black â”‚   34.1500 â”‚             33.1500 â”‚       34.1500 â”‚                        34.0900 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Negatives - White â”‚   54.2400 â”‚             57.5600 â”‚       55.2800 â”‚                        56.7300 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Negatives - Black â”‚   34.6700 â”‚             31.5100 â”‚       30.9800 â”‚                        30.5600 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Logistic Regression â”‚ F1-score                â”‚    0.6000 â”‚              0.6033 â”‚        0.6033 â”‚                         0.5721 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ F1-standard deviation   â”‚    0.0200 â”‚              0.0208 â”‚        0.0209 â”‚                         0.0282 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Positives - White â”‚   11.2900 â”‚             11.2900 â”‚       11.2900 â”‚                        11.4200 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Positives - Black â”‚   37.7700 â”‚             37.7200 â”‚       37.7700 â”‚                        37.6600 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Negatives - White â”‚   65.4200 â”‚             65.4200 â”‚       65.4200 â”‚                        65.5300 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                     â”‚ False Negatives - Black â”‚   28.6700 â”‚             28.6700 â”‚       28.6700 â”‚                        28.8800 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "header = [\"\", \"\", \"Vanilla\", \"Randomized Search\",  \"Grid Search\", \"Fairlearn CorrelationRemover\"]\n",
        "\n",
        "data = [\n",
        "    [\"XG-Boost\", \"F1-score\", F1_mean_vanilla, F1_mean_rs, F1_mean_gs, F1_mean_gs_cr],\n",
        "    [\"\", \"F1-standard deviation\", F1_std_vanilla, F1_std_rs, F1_std_gs, F1_std_gs_cr],\n",
        "    [\"\", \"False Positives - White\", FP_white_vanilla, FP_white_rs, FP_white_gs, FP_white_gs_cr],\n",
        "    [\"\", \"False Positives - Black\", FP_black_vanilla, FP_black_rs, FP_black_gs, FP_black_gs_cr],\n",
        "    [\"\", \"False Negatives - White\", FN_white_vanilla, FN_white_rs, FN_white_gs, FN_white_gs_cr],\n",
        "    [\"\", \"False Negatives - Black\", FN_black_vanilla, FN_black_rs, FN_black_gs, FN_black_gs_cr],\n",
        "    [\"Logistic Regression\", \"F1-score\", F1_mean_LR8, F1_mean_lr_rs, F1_mean_lr_gs, F1_mean_lr_gs_cr],\n",
        "    [\"\", \"F1-standard deviation\", F1_std_LR8, F1_std_lr_rs, F1_std_lr_gs, F1_std_lr_gs_cr],\n",
        "    [\"\", \"False Positives - White\", FP_white_LR8, FP_white_lr_rs, FP_white_lr_gs, FP_white_lr_gs_cr],\n",
        "    [\"\", \"False Positives - Black\", FP_black_LR8, FP_black_lr_rs, FP_black_lr_gs, FP_black_lr_gs_cr],\n",
        "    [\"\", \"False Negatives - White\", FN_white_LR8, FN_white_lr_rs, FN_white_lr_gs, FN_white_lr_gs_cr],\n",
        "    [\"\", \"False Negatives - Black\", FN_black_LR8, FN_black_lr_rs, FN_black_lr_gs, FN_black_lr_gs_cr]\n",
        "]\n",
        "\n",
        "print(tabulate(data, headers=header, tablefmt=\"fancy_grid\", floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2VkuHChxomC"
      },
      "source": [
        "## Statistical Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1T1Bm7Byni5"
      },
      "source": [
        "#### Mann-Whitney U test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVL2fRTvuqZc"
      },
      "source": [
        "The Mann-Whitney U test can show if there is a statistically significant difference between two groups. The null hypothesis is that the two groups are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8qFGpgmiF1A",
        "outputId": "bdd624dd-f6a5-4346-ddde-51b060ae095a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U statistic:  87.0\n",
            "P-value:  0.005776978583674299\n",
            "Reject the null hypothesis: The models' results are significantly different.\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# create arrays\n",
        "f1_scores_xgb = []\n",
        "f1_scores_lr = []\n",
        "\n",
        "# for each fold append the F1-score\n",
        "for i in range(kf.get_n_splits()):\n",
        "    f1_scores_xgb.append(grid_search_cr.cv_results_[f'split{i}_test_score'])\n",
        "    f1_scores_lr.append(grid_search_lr_cr.cv_results_[f'split{i}_test_score'])\n",
        "\n",
        "# flattening the list so it is only one array\n",
        "f1_scores_xgb = [item for sublist in f1_scores_xgb for item in sublist]\n",
        "f1_scores_lr = [item for sublist in f1_scores_lr for item in sublist]\n",
        "\n",
        "# mann-whitney u test\n",
        "statistics_mwu, pvalue_mwu = mannwhitneyu(f1_scores_xgb, f1_scores_lr, alternative='two-sided')\n",
        "\n",
        "# print results\n",
        "print(\"U statistic: \", statistics_mwu)\n",
        "print(\"P-value: \", pvalue_mwu)\n",
        "\n",
        "# interpretation\n",
        "alpha = 0.05\n",
        "if pvalue_mwu < alpha:\n",
        "    print(\"Reject the null hypothesis: The models' results are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference between the models' results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TSOlej9B6Y"
      },
      "source": [
        "#### ttest_ind_from_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgaqa97V-u4i"
      },
      "source": [
        "This is a T-test for means of two independent samples. A T-test allows to test for hypotheses within a t-distributed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7133BxfQAadE",
        "outputId": "a6014ab1-ac2e-49a2-dc39-9eaa56d791e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T-statistic:  [6.18983832]\n",
            "P-value:  [7.65755012e-06]\n",
            "Reject the null hypothesis: The models' values are significantly different.\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import ttest_ind_from_stats\n",
        "\n",
        "# ttest_ind_from_stats test\n",
        "statistics_tt, pvalue_tt = ttest_ind_from_stats(F1_mean_gs_cr, F1_std_gs_cr, 10, F1_mean_lr_gs_cr, F1_std_lr_gs_cr, 10, equal_var=True, alternative='two-sided')\n",
        "\n",
        "# print results\n",
        "print(\"T-statistic: \", statistics_tt)\n",
        "print(\"P-value: \", pvalue_tt)\n",
        "\n",
        "# interpretation\n",
        "alpha = 0.05\n",
        "if pvalue_tt < alpha:\n",
        "    print(\"Reject the null hypothesis: The models' values are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference between the models' values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi6eZB13iF1F"
      },
      "source": [
        "Quellen:\n",
        "\n",
        "RegenerativeToday. (18.05.2022). Step by Step Tutorial on Logistic Regression in Python | sklearn |Jupyter Notebook [Video]. Youtube. https://www.youtube.com/watch?v=bSXIbCZNBw0\n",
        "\n",
        "Dressel, A. & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. Science Advances, 4 (1), eaao5580. https://doi.org/10.1126/sciadv.aao5580\n",
        "\n",
        "Ryan Nolan Data. (28.08.2023). A Comprehensive Guide to Cross-Validation with Scikit-Learn and Python [Video]. Youtube. https://www.youtube.com/watch?v=glLNo1ZnmPA&list=PLcQVY5V2UY4LNmObS0gqNVyNdVfXnHwu8&index=14\n",
        "\n",
        "Scikit-Learn. (n.d.). User Guide. https://scikit-learn.org/stable/user_guide.html\n",
        "\n",
        "Scikit-Learn. (n.d.). API Reference. https://scikit-learn.org/stable/api/index.html\n",
        "\n",
        "Geeks for Geeks. (2022). How to make a table in Python?. https://www.geeksforgeeks.org/how-to-make-a-table-in-python/\n",
        "\n",
        "Stackoverflow. (2016). Python's tabulate number of decimal. https://stackoverflow.com/questions/37079957/pythons-tabulate-number-of-decimal\n",
        "\n",
        "Stackoverflow. (2018). How can I standardize only numeric variables in an sklearn pipeline?. https://stackoverflow.com/questions/48673402/how-can-i-standardize-only-numeric-variables-in-an-sklearn-pipeline\n",
        "\n",
        "Velarde, G., Weichert, M., Deshmunkh, A., Deshmane, S., Sudhir, A., Sharma, K. & Joshi, V. (2024). Tree boosting methods for balanced and imbalanced classification and their robustness over time in risk assessment. Intelligent Systems with Applications. 22, 200354. https://doi.org/10.1016/j.iswa.2024.200354\n",
        "\n",
        "Weerts, H. (19.06.2024). An Introduction to Responsible Machine Learning. GitHub. https://hildeweerts.github.io/responsiblemachinelearning/index.html\n",
        "\n",
        "Stackoverflow.(2018). getting the confusion matrix for each cross validation fold. https://stackoverflow.com/questions/49587820/getting-the-confusion-matrix-for-each-cross-validation-fold\n",
        "\n",
        "Fairlearn. (2023). User Guide. https://fairlearn.org/v0.10/user_guide/index.html\n",
        "\n",
        "Qiao, F. (08.01.2019). Logistic Regression Model Tuning with scikit-learn â€” Part 1: Comparison of metrics along the model tuning process. Towards Data Science.\n",
        "https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\n",
        "\n",
        "Group, M. (21.05.2023). A Comprehensive Analysis of Hyperparameter Optimization in Logistic Regression Models. Level Up Coding. https://levelup.gitconnected.com/a-comprehensive-analysis-of-hyperparameter-optimization-in-logistic-regression-models-521564c1bfc0\n",
        "\n",
        "Stackoverflow. (2014). Fine-tuning parameters in Logistic Regression. https://stackoverflow.com/questions/21816346/fine-tuning-parameters-in-logistic-regression\n",
        "\n",
        "SciPy. (n.d.). User Guide. https://docs.scipy.org/doc/scipy/tutorial/index.html\n",
        "\n",
        "SciPy. (n.d.). API Reference. https://docs.scipy.org/doc/scipy/reference/index.html\n",
        "\n",
        "Stackoverflow. (2014). How does the list comprehension to flatten a python list work? [duplicate]. https://stackoverflow.com/questions/25674169/how-does-the-list-comprehension-to-flatten-a-python-list-work\n",
        "\n",
        "Weerts, H., DudÃ­k, M., Edgar, R., Jalali, A., Lutz, R. & Madaio, M. (2023). Fairlearn: Assessing and Improving Fairness of AI Systems. Journal of Machine Learning Research, 24 (257), 1-8. http://jmlr.org/papers/v24/23-0389.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
